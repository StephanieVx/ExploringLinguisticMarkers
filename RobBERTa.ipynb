{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "main_folder = 'Data/MD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "for file in Path(main_folder).iterdir():\n",
    "    with open(file, encoding = \"ISO-8859-1\") as f:\n",
    "        #lines = [x.decode('utf8').strip() for x in f.readlines()]\n",
    "        results[\"file_name\"].append(file.name)\n",
    "        results[\"Text\"].append(f.read())\n",
    "        \n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "for file in Path('Data/NoMD').iterdir():\n",
    "    with open(file, encoding = \"ISO-8859-1\") as file_open:\n",
    "        results[\"file_name\"].append(file.name)\n",
    "        results[\"Text\"].append(file_open.read())\n",
    "        \n",
    "df2 = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Disorder']='nomd'\n",
    "df['Disorder']='md'\n",
    "result1 = pd.concat([df, df2])\n",
    "del result1['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmentald = pd.read_excel(r'MentalDisorders/LIWCresultstotal.xlsx')\n",
    "df.columns = ['Filename','Text']\n",
    "result = pd.merge(df, dfmentald, on='Filename', how = 'outer').fillna(0)\n",
    "del result['Filename']\n",
    "result1 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1['Disorder'] = pd.factorize(result1['Disorder'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1['Text'] = result1['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "def split_train_test(result, test_size=0.2, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result1[['Text']], \n",
    "                                                       result1['Disorder'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.merge(Y_train, X_train, on='index', how = 'outer').fillna(0)\n",
    "del result2['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = pd.merge(Y_test, X_test, on='index', how = 'outer').fillna(0)\n",
    "del result3['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = 'MentalDisorders/'\n",
    "\n",
    "train_df = result2\n",
    "train_df.head()\n",
    "\n",
    "eval_df = result3\n",
    "eval_df.head()\n",
    "\n",
    "train_df['Disorder'] = pd.to_numeric(train_df['Disorder'])\n",
    "eval_df['Disorder'] = pd.to_numeric(eval_df['Disorder'])\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_df[\"Text\"].replace(r'\\n', ' ', regex=True),\n",
    "    'label':train_df['Disorder']\n",
    "})\n",
    "\n",
    "print(train_df.head())\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    'text': eval_df['Text'].replace(r'\\n', ' ', regex=True),\n",
    "    'label':eval_df['Disorder']\n",
    "})\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "model_args = ClassificationArgs(num_train_epochs= 3,  overwrite_output_dir= True, dataloader_num_workers = 0)\n",
    "\n",
    "# Create a TransformerModel\n",
    "model = ClassificationModel('roberta', 'pdelobelle/robbert-v2-dutch-base', use_cuda=False, num_labels = 2,\n",
    "                            args = model_args)\n",
    "#\n",
    "# Train the model\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model\n",
    "#result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result, model_outputs, wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "lst = []\n",
    "for arr in model_outputs:\n",
    "    lst.append(np.argmax(arr))\n",
    "true = eval_df['label'].tolist()\n",
    "predicted = lst\n",
    "sklearn.metrics.accuracy_score(true,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score\n",
    "lst = []\n",
    "for arr in model_outputs:\n",
    "    lst.append(np.argmax(arr))\n",
    "true = eval_df['label'].tolist()\n",
    "predicted = lst\n",
    "sklearn.metrics.cohen_kappa_score(true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_text\n",
    "import re\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "def tokenize_string(string):\n",
    "    return string.split()\n",
    "\n",
    "classifier = (model)\n",
    "\n",
    "\n",
    "explainer = lime.lime_text.LimeTextExplainer(\n",
    "    split_expression=tokenize_string,\n",
    "    bow=False,\n",
    "    class_names=[\"0\", \"1\"]\n",
    ")\n",
    "\n",
    "def fasttext_prediction_in_sklearn_format(classifier, texts):\n",
    "    res = []\n",
    "    labels, probabilities = classifier.predict(texts, 10)\n",
    "    for label, probs, text in zip(labels, probabilities, texts):\n",
    "        order = np.argsort(np.array(label))\n",
    "        res.append(probs[order])\n",
    "\n",
    "    return np.array(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = eval_df.iloc[27,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def predictor(texts):\n",
    "    predictions = model.predict(texts)\n",
    "    x = np.array(list(predictions)[1])\n",
    "    return np.apply_along_axis(softmax, 1, x)\n",
    "\n",
    "\n",
    "explainer = lime.lime_text.LimeTextExplainer(class_names=[\"0\", \"1\"], bow=False)\n",
    "\n",
    "str_to_predict = review\n",
    "exp = explainer.explain_instance(str_to_predict, predictor, num_features=10, num_samples=200, top_labels=2)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (model.predict(review))\n",
    "print (results[1][0])\n",
    "print(testset.iloc[27,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = exp.as_pyplot_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
